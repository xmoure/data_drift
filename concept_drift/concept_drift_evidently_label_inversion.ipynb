{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import ClassificationPreset\n",
    "from evidently.ui.workspace.cloud import CloudWorkspace\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "import mlflow\n",
    "from scipy.stats import wasserstein_distance\n",
    "from river.drift import PageHinkley\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set MLflow experiment name\n",
    "mlflow.set_experiment(\"Concept drift experiments inverting labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_squares = {\"g8\", \"b1\", \"e5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_label_mapping = {\n",
    "    \"bb\": 1, \"bk\": 1, \"bn\": 1, \"bp\": 1, \"bq\": 1, \"br\": 1,  # Black pieces (occupied)\n",
    "    \"wb\": 1, \"wk\": 1, \"wn\": 1, \"wp\": 1, \"wq\": 1, \"wr\": 1,  # White pieces (occupied)\n",
    "    \"empty\": 0  # Empty square\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model\n",
    "model = load_model('/Users/ximenamoure/Desktop/drift_last/models/mobilenet_v2_occupancy_tf216.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_path = \"/Users/ximenamoure/Desktop/drift_last/reference_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = \"/Users/ximenamoure/Chess-Piece-Classification-Dataset/images/processed/occupancy/split0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_name = os.path.basename(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"bb\", \"bk\", \"bn\", \"bp\", \"bq\", \"br\", \"wb\", \"wk\", \"wn\", \"wp\", \"wq\", \"wr\", \"empty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {category: 1 for category in categories if category != \"empty\"}\n",
    "label_mapping[\"empty\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images and their labels\n",
    "def load_images_and_labels(base_path, categories):\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            images.append(img)\n",
    "            labels.append(label_mapping[category])\n",
    "            image_paths.append(img_path)\n",
    "\n",
    "\n",
    "    return np.array(images), np.array(labels), np.array(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels_with_inversion(base_path, categories):\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_name)\n",
    "\n",
    "            # Load and process the image\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            images.append(img)\n",
    "\n",
    "            # Determine original label\n",
    "            original_label = original_label_mapping[category]\n",
    "\n",
    "            # Extract the square position from the filename\n",
    "            square_position = img_name.split('_')[-1].split('.')[0]  # Gets \"g8\" from filename\n",
    "\n",
    "            # Check if the square position matches the target for inversion\n",
    "            if square_position in target_squares:\n",
    "                # Invert the label: occupied (1) becomes empty (0) and vice versa\n",
    "                inverted_label = 1 - original_label\n",
    "                labels.append(inverted_label)\n",
    "            else:\n",
    "                labels.append(original_label)\n",
    "\n",
    "            image_paths.append(img_path)\n",
    "\n",
    "    return np.array(images), np.array(labels), np.array(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, images, batch_size=32, threshold = 0.5):\n",
    "    num_images = len(images)\n",
    "    predictions = []\n",
    "\n",
    "    # Process in batches\n",
    "    for start in range(0, num_images, batch_size):\n",
    "        end = min(start + batch_size, num_images)\n",
    "        batch_images = images[start:end]\n",
    "        batch_predictions = model.predict(batch_images)\n",
    "        print(\"predictions\", batch_predictions)\n",
    "\n",
    "        # Convert probabilities to binary labels based on threshold\n",
    "        batch_labels = (batch_predictions >= threshold).astype(int)\n",
    "\n",
    "        # Flatten the array and add to the list of predictions\n",
    "        predictions.extend(batch_labels.flatten())\n",
    "        print(f\"Processed {end}/{num_images} images ({(end / num_images) * 100:.2f}% complete)\")\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores(model, images, batch_size=32):\n",
    "    num_images = len(images)\n",
    "    scores = []\n",
    "    for start in range(0, num_images, batch_size):\n",
    "        end = min(start + batch_size, num_images)\n",
    "        batch_images = images[start:end]\n",
    "        batch_scores = model.predict(batch_images)\n",
    "        scores.extend(batch_scores.flatten())\n",
    "\n",
    "        print(f\"Processed {end}/{num_images} images ({(end / num_images) * 100:.2f}% complete)\")\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ground_truth, predictions):\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    f1 = f1_score(ground_truth, predictions)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate batch-wise error rates\n",
    "def batch_error_rates(model, images, labels, batch_size=32):\n",
    "    batch_errors = []\n",
    "    for start in range(0, len(images), batch_size):\n",
    "        end = min(start + batch_size, len(images))\n",
    "        batch_images = images[start:end]\n",
    "        batch_labels = labels[start:end]\n",
    "\n",
    "        # Get predictions for the batch\n",
    "        batch_preds = model.predict(batch_images)\n",
    "        batch_preds = (batch_preds >= 0.5).astype(int).flatten()\n",
    "\n",
    "        # Calculate error rate for this batch\n",
    "        incorrect = np.sum(batch_preds != batch_labels)\n",
    "        error_rate = incorrect / len(batch_labels)\n",
    "        batch_errors.append(error_rate)\n",
    "\n",
    "    return batch_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cusum_test(error_rates, threshold=0.1):\n",
    "    \"\"\"\n",
    "    CUSUM test for detecting shifts in the error rate sequence.\n",
    "\n",
    "    Args:\n",
    "        error_rates (list or array): Sequence of error rates.\n",
    "        threshold (float): Threshold for detecting drift.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if drift is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    cumulative_sum = 0\n",
    "    drift_detected = False\n",
    "\n",
    "    for error in error_rates:\n",
    "        # Calculate cumulative sum of deviations\n",
    "        cumulative_sum += (error - np.mean(error_rates))\n",
    "        if abs(cumulative_sum) > threshold:\n",
    "            drift_detected = True\n",
    "            print(\"Concept Drift Detected by CUSUM Test\")\n",
    "            break  # Exit loop on drift detection\n",
    "        if cumulative_sum < 0:\n",
    "            cumulative_sum = 0\n",
    "\n",
    "    if not drift_detected:\n",
    "        print(\"No Concept Drift Detected by CUSUM Test\")\n",
    "    return drift_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_hinkley_test(data, threshold=0.1, delta=0.005):\n",
    "    \"\"\"\n",
    "    Page-Hinkley test for detecting sudden changes in a sequence.\n",
    "\n",
    "    Args:\n",
    "        data (list or array): Sequence of error rates or other metrics.\n",
    "        threshold (float): Threshold for detecting drift.\n",
    "        delta (float): Small constant to reduce the sensitivity to small changes.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if drift is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    mean_est = np.mean(data)\n",
    "    cumulative_sum = 0\n",
    "    min_sum = 0\n",
    "    drift_detected = False\n",
    "\n",
    "    for i, point in enumerate(data):\n",
    "        cumulative_sum += point - mean_est - delta\n",
    "        min_sum = min(min_sum, cumulative_sum)\n",
    "        if cumulative_sum - min_sum > threshold:\n",
    "            drift_detected = True\n",
    "            print(f\"Concept Drift Detected by Page-Hinkley Test at point {i}\")\n",
    "            break  # Exit loop on drift detection\n",
    "\n",
    "    if not drift_detected:\n",
    "        print(\"No Concept Drift Detected by Page-Hinkley Test\")\n",
    "    return drift_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ref, labels_ref, img_paths_ref = load_images_and_labels(ref_path, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels_inverted, image_paths = load_images_and_labels_with_inversion(ref_path, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path, label in zip(image_paths, labels_inverted):\n",
    "    square_position = img_path.split('_')[-1].split('.')[0]\n",
    "    print(f\"Image: {img_path}, Square: {square_position}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/Users/ximenamoure/Desktop/drift_last/models/mobilenet_v2_occupancy_tf216.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_threshold = 0.12\n",
    "ws_threshold_prev = 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"ref_split_label_inversion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_drop_threshold = 0.1 # for a 10% decrease\n",
    "f1_drop_threshold = 0.1 # for a 10% decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start MLflow Run ---\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.set_tag(\"mlflow.runName\", \"label_inversion\")\n",
    "    print(\"Processing and logging reference split...\")\n",
    "    images_ref, labels_ref, img_paths_ref = load_images_and_labels(ref_path, categories)\n",
    "    predictions_ref = get_model_predictions(model, images_ref)\n",
    "    batch_errors_ref = batch_error_rates(model, images_ref, labels_ref)\n",
    "    accuracy_ref, precision_ref, recall_ref, f1_ref = get_metrics(labels_ref, predictions_ref)\n",
    "\n",
    "    # Log reference metrics and batch errors as an artifact\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy_ref\": accuracy_ref,\n",
    "        \"precision_ref\": precision_ref,\n",
    "        \"recall_ref\": recall_ref,\n",
    "        \"f1_ref\": f1_ref\n",
    "    })\n",
    "    images_new = images_ref\n",
    "    labels_new = labels_inverted\n",
    "    predictions_new = get_model_predictions(model, images_new)\n",
    "    batch_errors_new = batch_error_rates(model, images_new, labels_new)\n",
    "    accuracy_new, precision_new, recall_new, f1_new = get_metrics(labels_new, predictions_new)\n",
    "    # Log new split metrics and error rates to MLflow\n",
    "    mlflow.log_metrics({\n",
    "            f\"{split_name}_accuracy\": accuracy_new,\n",
    "            f\"{split_name}_precision\": precision_new,\n",
    "            f\"{split_name}_recall\": recall_new,\n",
    "            f\"{split_name}_f1\": f1_new\n",
    "    })\n",
    "\n",
    "    # --- Long-Term Drift Detection with Reference Split ---\n",
    "    wd_ref = wasserstein_distance(batch_errors_ref, batch_errors_new)\n",
    "    # Detect drift based on threshold\n",
    "    drift_detected_ref = wd_ref > ws_threshold\n",
    "\n",
    "    print(\"drfit_detected\", drift_detected_ref)\n",
    "\n",
    "    mlflow.log_metric(f\"{split_name}_wasserstein_distance_ref\", wd_ref)\n",
    "    mlflow.log_metric(f\"{split_name}_drift_detected_ref\", int(drift_detected_ref))\n",
    "\n",
    "    #--- Generate and Log Evidently Report ---\n",
    "    reference_data = pd.DataFrame({'prediction': predictions_ref, 'target': labels_ref, 'dataset': 'reference'})\n",
    "    new_data = pd.DataFrame({'prediction': predictions_new, 'target': labels_new, 'dataset': 'new'})\n",
    "    classification_report = Report(metrics=[ClassificationPreset()])\n",
    "    classification_report.run(reference_data=reference_data, current_data=new_data)\n",
    "\n",
    "    # Save and log the Evidently report\n",
    "    report_file = f\"{split_name}_classification_report.html\"\n",
    "    classification_report.save_html(report_file)\n",
    "    mlflow.log_artifact(report_file)\n",
    "\n",
    "    results = classification_report.as_dict()\n",
    "    current_metrics = results[\"metrics\"][0][\"result\"][\"current\"]\n",
    "    reference_metrics = results[\"metrics\"][0][\"result\"][\"reference\"]\n",
    "    accuracy_current = current_metrics[\"accuracy\"]\n",
    "    f1_current = current_metrics[\"f1\"]\n",
    "    recall_current = current_metrics[\"recall\"]\n",
    "    accuracy_reference = reference_metrics[\"accuracy\"]\n",
    "    f1_reference = reference_metrics[\"f1\"]\n",
    "\n",
    "    accuracy_significant_drop = (accuracy_reference - accuracy_current) > (accuracy_reference * accuracy_drop_threshold)\n",
    "    f1_significant_drop = (f1_reference - f1_current) > (f1_reference * f1_drop_threshold)\n",
    "    print(\"Significant Accuracy Drop:\", accuracy_significant_drop)\n",
    "    print(\"Significant F1 Score Drop:\", f1_significant_drop)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy_significant_drop\", int(accuracy_significant_drop))\n",
    "    mlflow.log_metric(\"f1_significant_drop\", int(f1_significant_drop))\n",
    "    mlflow.log_metric(f\"{split_name}_accuracy\", accuracy_current)\n",
    "    mlflow.log_metric(f\"{split_name}_f1\", f1_current)\n",
    "    mlflow.log_metric(f\"{split_name}_recall\", recall_current)\n",
    "\n",
    "    target_squares_str = \",\".join(target_squares)\n",
    "    mlflow.log_param(\"target_squares\", target_squares_str)\n",
    "\n",
    "\n",
    "    print(\"Run completed and all data logged to MLflow!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow5",
   "language": "python",
   "name": "tensorflow5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
